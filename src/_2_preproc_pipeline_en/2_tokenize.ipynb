{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:19.952842Z",
     "start_time": "2024-05-17T16:52:19.703615Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data_path = \"data_preprocessed/1_txt_cleaning_eu_en.csv\"\n",
    "# output_path = \"data_preprocessed/2_tokenize_eu_en.csv\"\n",
    "data_path = \"data_preprocessed/1_txt_cleaning_ch_en.csv\"\n",
    "output_path = \"data_preprocessed/2_tokenize_ch_en.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:19.969496Z",
     "start_time": "2024-05-17T16:52:19.953870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24992 entries, 0 to 24991\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             24992 non-null  object\n",
      " 1   pub_year       24992 non-null  int64 \n",
      " 2   text_preproc1  24992 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 585.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:21.805033Z",
     "start_time": "2024-05-17T16:52:19.970161Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.moduls.pipeline_en import spacy_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:29.959917Z",
     "start_time": "2024-05-17T16:52:21.805824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfc95d64d56422899b32c56a0aaeb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SPACY_TOKENIZE DONE:   0%|          | 0/24992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from parallel_pandas import ParallelPandas\n",
    "\n",
    "#initialize parallel-pandas\n",
    "ParallelPandas.initialize(n_cpu=10, split_factor=1, disable_pr_bar=False)\n",
    "\n",
    "df['text_preproc2'] = df['text_preproc1'].p_apply(spacy_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:29.983549Z",
     "start_time": "2024-05-17T16:52:29.961603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_preproc1</th>\n",
       "      <th>text_preproc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METHOD FOR DYNAMIC TARGET TRACKING BY LEGGED ROBOT. Disclosed in the present invention is a method for dynamic target tracking by a legged robot. Firstly, images and videos in walking processes of pedestrians wearing masks, not wearing masks, and incorrectly wearing masks are collected to build a data set used for pedestrian mask wearing detection. Secondly, a target recognition module based on a recurrent neural network is used to determine whether a pedestrian wears a mask, and acquire face position coordinates of a pedestrian not wearing a mask in a current frame. Thirdly, a residual network is used as a reference network to extract a face semantic feature of the pedestrian not wearing a mask in the current image frame, and predict a face semantic feature of the pedestrian in a next image frame. Finally, a target tracking module based on a siamese network is designed to, by calculating a correlation coefficient between pedestrian face position semantic feature mapping in the current frame and the next frame, track the pedestrian.</td>\n",
       "      <td>method dynamic target tracking legged disclose present invention method dynamic target tracking legged image video walk process pedestrian wear mask collect build data set pedestrian mask wear detection target recognition module base recurrent neural network determine pedestrian wear mask acquire face position coordinate pedestrian wear mask current frame residual network reference network extract face semantic feature pedestrian wear mask current image frame predict face semantic feature pedestrian image frame target tracking module base siamese network design calculate correlation coefficient pedestrian face position semantic feature mapping current frame track pedestrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPERATION AND MAINTENANCE ROBOT. An operation and maintenance robot, comprising: an operation and maintenance robot body, first support members each comprising a first support body and a connection part, the connection part being fixedly arranged on the first support body, and the first support body being fixed to the operation and maintenance robot body, second support members each comprising a second support body, a first sliding part and a second sliding part, the second support body comprising a first side surface and a second side surface, the first sliding part being fixedly arranged on the first side surface, the second sliding part being fixedly arranged on the second side surface, and the first sliding part being slidably connected to the connection part, such that the second support members are close to or away from the operation and maintenance robot body relative to the first support.</td>\n",
       "      <td>operation maintenance comprise operation maintenance body support member comprise support body connection arrange support body fix operation maintenance body second support member comprise second support body slide second support body comprise surface second surface slide arrange surface second slide arrange second surface slide connect connection second support member close operation maintenance body relative support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAINTENANCE BASE STATION AND CLEANING ROBOT SYSTEM. A maintenance base station and a cleaning robot system. The maintenance base station comprises: at least one functional module, a human machine interaction component, configured to receive a user input, the human machine interaction component determining a human machine interaction signal according to the user input, the human machine interaction signal indicating a device type and a device task type, and a device indicated by the device type at least comprising a cleaning robot or the functional module of the maintenance base station, and a controller, electrically connected to the at least one functional module and the human machine interaction component, the controller being configured to receive the human machine interaction signal and, according to the human machine interaction signal, instruct a corresponding device to work.</td>\n",
       "      <td>maintenance base station cleaning system maintenance base station comprise functional module human machine interaction component configure receive user input human machine interaction component determine human machine interaction signal accord user input human machine interaction signal indicate device type device task type device indicate device type comprise cleaning functional module maintenance base station controller connect functional module human machine interaction component controller configure receive human machine interaction signal accord human machine interaction signal instruct corresponding device work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONTROL METHOD FOR SWIMMING POOL CLEANING ROBOT, AND SWIMMING POOL CLEANING ROBOT. A control method for a swimming pool cleaning robot, and a swimming pool cleaning robot. Specifically, by means of measuring the attitude angle of a swimming pool cleaning robot in real time, whether the swimming pool cleaning robot has been placed in a swimming pool and is in a sinking state can be identified accurately and timely on the basis of the attitude angle, and after it is identified that the swimming pool cleaning robot is in the sinking state, a water pump electric motor of the swimming pool cleaning robot is controlled to alternately turn on and off, such that the head of the swimming pool cleaning robot that originally faces upwards is pressed down gradually, and it can be effectively ensured that the swimming pool cleaning robot smoothly sinks to the bottom of the swimming pool. In addition, while the water pump electric motor is intermittently turned on and off, the swimming pool cleaning robot can intermittently shake as the water pump electric motor.</td>\n",
       "      <td>control method swimming pool cleaning mean measure attitude angle swimming pool clean real time swimming pool cleaning place swimming pool sink state identify basis attitude angle identify swimming pool cleaning sink state water pump electric motor swimming pool cleaning control turn head swimming pool cleaning face press ensure swimming pool clean sink swimming pool addition water pump electric motor turn swimming pool cleaning shake water pump electric motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROBOT TEACHING METHOD AND APPARATUS, ELECTRONIC DEVICE, AND STORAGE MEDIUM. A robot teaching method and apparatus, an electronic device, and a storage medium. The method is used for teaching a robot of which the operation range can cover a plurality of operation stations, wherein the plurality of operation stations are configured to store operation objects. The method comprises: determining target operation stations specified by a user, wherein the target operation stations are some of the plurality of operation stations, teaching the robot on the basis of the target operation stations to obtain a first teaching result, and according to the first teaching result and arrangement information of the plurality of operation stations, obtaining a second teaching result of the other operation stations different from the target operation stations among the plurality of operation stations. After teaching is performed for some of operation stations, a teaching result of the other operation stations can be calculated, thereby effectively reducing the workload of.</td>\n",
       "      <td>teaching method apparatus electronic device storage medium method teach operation range cover plurality operation station configure store operation object method comprise determine target operation station specify user target operation station plurality operation station teach basis target operation station obtain teaching result accord teaching result arrangement information plurality operation station obtain second teaching result operation station different target operation station plurality operation station teaching perform operation station teaching result operation station calculate reduce workload</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text_preproc1  \\\n",
       "0                      METHOD FOR DYNAMIC TARGET TRACKING BY LEGGED ROBOT. Disclosed in the present invention is a method for dynamic target tracking by a legged robot. Firstly, images and videos in walking processes of pedestrians wearing masks, not wearing masks, and incorrectly wearing masks are collected to build a data set used for pedestrian mask wearing detection. Secondly, a target recognition module based on a recurrent neural network is used to determine whether a pedestrian wears a mask, and acquire face position coordinates of a pedestrian not wearing a mask in a current frame. Thirdly, a residual network is used as a reference network to extract a face semantic feature of the pedestrian not wearing a mask in the current image frame, and predict a face semantic feature of the pedestrian in a next image frame. Finally, a target tracking module based on a siamese network is designed to, by calculating a correlation coefficient between pedestrian face position semantic feature mapping in the current frame and the next frame, track the pedestrian.   \n",
       "1                                                                                                                                                                 OPERATION AND MAINTENANCE ROBOT. An operation and maintenance robot, comprising: an operation and maintenance robot body, first support members each comprising a first support body and a connection part, the connection part being fixedly arranged on the first support body, and the first support body being fixed to the operation and maintenance robot body, second support members each comprising a second support body, a first sliding part and a second sliding part, the second support body comprising a first side surface and a second side surface, the first sliding part being fixedly arranged on the first side surface, the second sliding part being fixedly arranged on the second side surface, and the first sliding part being slidably connected to the connection part, such that the second support members are close to or away from the operation and maintenance robot body relative to the first support.   \n",
       "2                                                                                                                                                                                MAINTENANCE BASE STATION AND CLEANING ROBOT SYSTEM. A maintenance base station and a cleaning robot system. The maintenance base station comprises: at least one functional module, a human machine interaction component, configured to receive a user input, the human machine interaction component determining a human machine interaction signal according to the user input, the human machine interaction signal indicating a device type and a device task type, and a device indicated by the device type at least comprising a cleaning robot or the functional module of the maintenance base station, and a controller, electrically connected to the at least one functional module and the human machine interaction component, the controller being configured to receive the human machine interaction signal and, according to the human machine interaction signal, instruct a corresponding device to work.   \n",
       "3     CONTROL METHOD FOR SWIMMING POOL CLEANING ROBOT, AND SWIMMING POOL CLEANING ROBOT. A control method for a swimming pool cleaning robot, and a swimming pool cleaning robot. Specifically, by means of measuring the attitude angle of a swimming pool cleaning robot in real time, whether the swimming pool cleaning robot has been placed in a swimming pool and is in a sinking state can be identified accurately and timely on the basis of the attitude angle, and after it is identified that the swimming pool cleaning robot is in the sinking state, a water pump electric motor of the swimming pool cleaning robot is controlled to alternately turn on and off, such that the head of the swimming pool cleaning robot that originally faces upwards is pressed down gradually, and it can be effectively ensured that the swimming pool cleaning robot smoothly sinks to the bottom of the swimming pool. In addition, while the water pump electric motor is intermittently turned on and off, the swimming pool cleaning robot can intermittently shake as the water pump electric motor.   \n",
       "4  ROBOT TEACHING METHOD AND APPARATUS, ELECTRONIC DEVICE, AND STORAGE MEDIUM. A robot teaching method and apparatus, an electronic device, and a storage medium. The method is used for teaching a robot of which the operation range can cover a plurality of operation stations, wherein the plurality of operation stations are configured to store operation objects. The method comprises: determining target operation stations specified by a user, wherein the target operation stations are some of the plurality of operation stations, teaching the robot on the basis of the target operation stations to obtain a first teaching result, and according to the first teaching result and arrangement information of the plurality of operation stations, obtaining a second teaching result of the other operation stations different from the target operation stations among the plurality of operation stations. After teaching is performed for some of operation stations, a teaching result of the other operation stations can be calculated, thereby effectively reducing the workload of.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text_preproc2  \n",
       "0  method dynamic target tracking legged disclose present invention method dynamic target tracking legged image video walk process pedestrian wear mask collect build data set pedestrian mask wear detection target recognition module base recurrent neural network determine pedestrian wear mask acquire face position coordinate pedestrian wear mask current frame residual network reference network extract face semantic feature pedestrian wear mask current image frame predict face semantic feature pedestrian image frame target tracking module base siamese network design calculate correlation coefficient pedestrian face position semantic feature mapping current frame track pedestrian  \n",
       "1                                                                                                                                                                                                                                                                       operation maintenance comprise operation maintenance body support member comprise support body connection arrange support body fix operation maintenance body second support member comprise second support body slide second support body comprise surface second surface slide arrange surface second slide arrange second surface slide connect connection second support member close operation maintenance body relative support  \n",
       "2                                                            maintenance base station cleaning system maintenance base station comprise functional module human machine interaction component configure receive user input human machine interaction component determine human machine interaction signal accord user input human machine interaction signal indicate device type device task type device indicate device type comprise cleaning functional module maintenance base station controller connect functional module human machine interaction component controller configure receive human machine interaction signal accord human machine interaction signal instruct corresponding device work  \n",
       "3                                                                                                                                                                                                                            control method swimming pool cleaning mean measure attitude angle swimming pool clean real time swimming pool cleaning place swimming pool sink state identify basis attitude angle identify swimming pool cleaning sink state water pump electric motor swimming pool cleaning control turn head swimming pool cleaning face press ensure swimming pool clean sink swimming pool addition water pump electric motor turn swimming pool cleaning shake water pump electric motor  \n",
       "4                                                                        teaching method apparatus electronic device storage medium method teach operation range cover plurality operation station configure store operation object method comprise determine target operation station specify user target operation station plurality operation station teach basis target operation station obtain teaching result accord teaching result arrangement information plurality operation station obtain second teaching result operation station different target operation station plurality operation station teaching perform operation station teaching result operation station calculate reduce workload  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['text_preproc1', 'text_preproc2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:29.999422Z",
     "start_time": "2024-05-17T16:52:29.984102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             0 non-null      object\n",
      " 1   pub_year       0 non-null      int64 \n",
      " 2   text_preproc1  0 non-null      object\n",
      " 3   text_preproc2  0 non-null      object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df[df['text_preproc2'] == ''].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:30.014675Z",
     "start_time": "2024-05-17T16:52:30.000423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24992.000000\n",
       "mean        83.041333\n",
       "std         12.066089\n",
       "min         18.000000\n",
       "25%         77.000000\n",
       "50%         84.000000\n",
       "75%         91.000000\n",
       "max        126.000000\n",
       "Name: text_preproc2, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_preproc2.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T16:52:30.046721Z",
     "start_time": "2024-05-17T16:52:30.015675Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns='text_preproc1').to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_angew_programm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
